{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "#MODEL = \"mixtral:8x7b\"\n",
    "#MODEL = \"llama2\"\n",
    "MODEL = \"tinyllama\"\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001D595C5DFC0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#langchain to load chroma db data\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "from langchain.vectorstores import Chroma\n",
    "chroma_db = Chroma(persist_directory=\"./chroma_db\", \n",
    "                   embedding_function=embed_model,\n",
    "                   collection_name=\"DB_collection\")\n",
    "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents > 5:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents > 5:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is data lake?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context> > 4:chain:retrieve_documents] [69ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context> > 3:chain:RunnableParallel<context>] [70ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 2:chain:RunnableAssign<context>] [72ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context> > 12:chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context> > 12:chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"•Additional costs in the compute for the additional ETL work and the cost for•\\nstoring a copy of data you are already storing in a data warehouse where the\\nstorage costs are often greater.\\n•Additional copies of the data may be needed to populate data marts for•\\ndifferent business lines and even more copies as analysts create physical\\ncopies of data subsets in the form of BI extracts to speed up dashboards.\\nLeading to a web of data copies that are hard to govern, track and keep in\\nsync.\\nYou run analytics directly on the Data Lake\\nY ou’ d use query engines that support data lake workloads like Dremio, Presto,\\nApache Spark, Trino, Apache Impala and more to execute queries on the data\\nlake. These engines are generally well suited for read-only workloads. However,\\ndue to the limitations of the Hive table format, they ran into complexity when\\ntrying to update the data safely from the data lake.\\nSo the data lake and data warehouse each have their unique benefits and unique cons.\\nIt would be to our advantage to develop a new architecture that brings together all\\nthese benefits while minimizing all their faults, and that architecture is called a data\\nlakehouse.\\nEnter Data Lakehouse\\nWhile using a data warehouse gave us performance and ease of use, analytics on data\\nlakes gave us lower costs and reduced data drift from a complex web of data copies.\\nThe desire to thread the needle leads to great strides and innovation leading to what\\nwe now know as the data lakehouse.\\nWhat makes a data lakehouse truly unique are data lake table formats that eliminate\\nall the previous issues with the Hive table format. Y ou store the data in the same\\nplaces you would with a data lake, you use the query engines you would use with a\\ndata lake, your data is stored in the same formats it would be on a data lake, what\\ntruly transforms your world from a “read only” data to a “center of my data world”\\ndata lakehouse is the table format (refer to Figure 1-4). Table formats enabled better\\nconsistency, performance and ACID guarantees when working with data directly on\\nyour data lake storage leading to several value propositions.\\nFewer Copies, Less Dri/f_t\\nWith ACID guarantees and better performance you can now move workloads\\ntypically saved for the data warehouse like updates and other data manipulation.\\nIf you don’t have to move your data to the lakehouse you can have a more\\nstreamlined architecture with fewer copies. Fewer copies mean less storage costs,\\nless compute costs from moving data to a data warehouse, and better governance\\nof your data to maintain compliance with regulations and internal controls.\\n22 | Chapter 1: What Is Apache Iceberg?\\n\\nFigure 1-3. Technical components in a data lake\\nPros and Cons of a Data Lake\\nNo architectural pattern is perfect and that applies to data lakes. While data lakes\\nhave a lot of benefits like Lower Costs, the ability to store in open formats and handle\\nunstructured data; data lakes also have several disadvantages such as performance\\nissues, lack of ACID guarantees and lots of configuration. Y ou can see a summary of\\nthese pros and cons in Table 1-2.\\nPro: Lower Cost\\nThe costs of storing data and executing queries on a data lake are much lower\\nthan in a data warehouse. This makes a data lake particularly useful for enabling\\nanalytics on data that isn’t high enough priority to justify the cost of a data\\nwarehouse enabling a wider analytical reach.\\n20 | Chapter 1: What Is Apache Iceberg?\\n\\n• A data warehouse could only store structured data •\\n•Storage in a data warehouse is generally more expensive than on-prem Hadoop•\\nclusters or cloud object storage.\\nTo address these issues the goal was to have an alternative storage solution that was\\ncheaper and could store all our data. This is what’s called the data lake.\\nOriginally, you’ d use a Hadoop to allow you to use a cluster of inexpensive comput‐\\ners to store large amounts of structured and unstructured data. Although it wasn’t\\nenough to just be able to store all this data. Y ou’ d want to run analytics on it too.\\nThe Hadoop ecosystem included MapReduce, an analytics framework from which\\nyou’ d write analytics jobs in Java and run them on the cluster. Many analysts are more\\ncomfortable writing SQL than Java, so Hive was created to convert SQL statements\\ninto MapReduce jobs.\\nTo write SQL, a mechanism to distinguish which files in our storage are part of the\\ndataset or table we want to run the SQL against was needed. This resulted in the birth\\nof the Hive table format which recognized a directory and the files inside it as a table.\\nOver time. people moved away from using Hadoop clusters to using Cloud Object\\nstorage as it was easier to manage and cheaper to use. MapReduce also fell out of\\nfavor for other distributed query engines like Apache Spark, Presto, and Dremio.\\nWhat did stick around was the Hive table format which became the standard in the\\nspace for recognizing files in your storage as singular tables on which you can run\\nanalytics.\\nA distinguishing feature of the data lake as compared to the data warehouse is\\nthe ability to leverage different compute engines for different workloads. This is\\nimportant because there’s never been a silver bullet of a compute engine that is best\\nfor every workload. This is just inherent to the nature of computing since there are\\nalways tradeoffs, and what you decide to tradeoff determines what a given system is\\ngood for and what it is not as well suited for.\\nNote that in data lakes, there isn’t really any service that fulfills the needs of the\\nstorage engine function. Generally the compute engine decides how to write the data,\\nthen the data is usually never revisited and optimized, unless rewriting entire tables\\nor partitions which is usually done on an ad-hoc basis. Refer to Figure 1-3 to see how\\nthe components of a data lake interact with one another.\\nData Lake | 19\\n\\nPro: Store Data in Open Formats\\nIn a data lake you can store the data in any file format you like unlike data\\nwarehouses where you have no say in how the data is stored, which would\\ntypically be a proprietary format built for that particular data warehouse. This\\nallows you to have more control over the data and consume the data in a greater\\nvariety of tools that can support these open formats.\\nPro: Handle Unstructured Data\\nData warehouses can’t handle unstructured data, so if you wanted to run analyt‐\\nics on unstructured data the data lake was the only option.\\nCon: Performance\\nSince each component of a data lake is decoupled, many of the optimizations\\nthat can exist in tightly coupled systems are absent. While they can be recreated,\\nit requires a lot of effort and engineering performant to cobble the components\\n(storage, file format, table format, engines) in a way to give you the comparable\\nperformance of a data warehouse. This made data lakes undesirable for high\\npriority data analytics where performance and time mattered.\\nCon: Lots of Con/f_iguration\\nAs previously mentioned, creating a tighter coupling of your chosen components\\nwith the level of optimizations you’ d expect from a data warehouse would require\\nsignificant engineering. This would result in a need for lots of data engineers to\\nconfigure all these tools, which can also be costly.\\nTable 1-2. Pros and cons of a data lake\\nPros Cons\\n• Lower Cost\\n• Store Data in Open Formats\\n• Handle unstructured data• Performance\\n• Lack of ACID Guarantees\\n• Lots of Con/f_igurations\\nShould I Run Analytics on the Data Lake or Data\\nWarehouse?\\nWhile Data Lakes provided a great place to land all your structured and unstructured\\ndata, there were still imperfections. After running ETL to land your data in your data\\nlake you’ d generally take one of two tracks when running analytics.\\nA Subset of Data goes to the Data Warehouse\\nY ou’ d set up an additional ETL pipeline to create a copy of a curated subset of\\ndata that is for high priority for analytics and store it in the warehouse to get the\\nperformance and flexibility of the data warehouse.\\nThis results in several issues:\\nShould I Run Analytics on the Data Lake or Data Warehouse? | 21\\n\\ndirectly without risking the system’s corruption. The details of how the data is stored\\nare abstracted away, and users take for granted that the platform knows where the\\ndata for a specific table is located and how to access it.\\nHowever, in today’s big data world, relying on a single closed engine to manage all\\naccess to the underlying data is no longer practical, as traditional RDBMSs are no\\nlonger sufficient.\\nIn a data lake, all your data is stored as files in some storage solution (e.g., Amazon\\nS3, Azure’s ADLS, Google’s GCS), so a single table may be made of dozens, hundreds,\\nor even thousands of individual files on that storage. When using SQL with our\\nfavorite analytical tools or writing ad hoc scripts in languages like Java, Scala, Python,\\nand Rust, we wouldn’t want to constantly define which of these files are in the table\\nand which of them aren’t. Not only would this be tedious but it would also likely lead\\nto inconsistency across different uses of the data.\\nSo the solution was to create a standard method of understanding “what data is in\\nthis table” for data lakes.\\nFigure 1-5. [Caption to come]\\nEarly Data Lake Table Formats\\nWhen it came to the world of running analytics on Hadoop data lakes, the Map‐\\nReduce framework was used which required users to write complex and tedious\\njava jobs, which wasn’t accessible to many analysts. Facebook, feeling the pain of\\nthis situation developed a framework called Hive in 2009. Hive provided a key\\nbenefit to make analytics on Hadoop much easier, the ability to write SQL instead of\\nMapReduce jobs directly.\\nThe Hive framework would take SQL statements and then convert them into Map‐\\nReduce jobs that can be executed. In order to write SQL statements, there had to\\nbe a mechanism for understanding what data on your Hadoop storage represented\\nEarly Data Lake Table Formats | 25\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs > 11:chain:RunnableParallel<context>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"•Additional costs in the compute for the additional ETL work and the cost for•\\nstoring a copy of data you are already storing in a data warehouse where the\\nstorage costs are often greater.\\n•Additional copies of the data may be needed to populate data marts for•\\ndifferent business lines and even more copies as analysts create physical\\ncopies of data subsets in the form of BI extracts to speed up dashboards.\\nLeading to a web of data copies that are hard to govern, track and keep in\\nsync.\\nYou run analytics directly on the Data Lake\\nY ou’ d use query engines that support data lake workloads like Dremio, Presto,\\nApache Spark, Trino, Apache Impala and more to execute queries on the data\\nlake. These engines are generally well suited for read-only workloads. However,\\ndue to the limitations of the Hive table format, they ran into complexity when\\ntrying to update the data safely from the data lake.\\nSo the data lake and data warehouse each have their unique benefits and unique cons.\\nIt would be to our advantage to develop a new architecture that brings together all\\nthese benefits while minimizing all their faults, and that architecture is called a data\\nlakehouse.\\nEnter Data Lakehouse\\nWhile using a data warehouse gave us performance and ease of use, analytics on data\\nlakes gave us lower costs and reduced data drift from a complex web of data copies.\\nThe desire to thread the needle leads to great strides and innovation leading to what\\nwe now know as the data lakehouse.\\nWhat makes a data lakehouse truly unique are data lake table formats that eliminate\\nall the previous issues with the Hive table format. Y ou store the data in the same\\nplaces you would with a data lake, you use the query engines you would use with a\\ndata lake, your data is stored in the same formats it would be on a data lake, what\\ntruly transforms your world from a “read only” data to a “center of my data world”\\ndata lakehouse is the table format (refer to Figure 1-4). Table formats enabled better\\nconsistency, performance and ACID guarantees when working with data directly on\\nyour data lake storage leading to several value propositions.\\nFewer Copies, Less Dri/f_t\\nWith ACID guarantees and better performance you can now move workloads\\ntypically saved for the data warehouse like updates and other data manipulation.\\nIf you don’t have to move your data to the lakehouse you can have a more\\nstreamlined architecture with fewer copies. Fewer copies mean less storage costs,\\nless compute costs from moving data to a data warehouse, and better governance\\nof your data to maintain compliance with regulations and internal controls.\\n22 | Chapter 1: What Is Apache Iceberg?\\n\\nFigure 1-3. Technical components in a data lake\\nPros and Cons of a Data Lake\\nNo architectural pattern is perfect and that applies to data lakes. While data lakes\\nhave a lot of benefits like Lower Costs, the ability to store in open formats and handle\\nunstructured data; data lakes also have several disadvantages such as performance\\nissues, lack of ACID guarantees and lots of configuration. Y ou can see a summary of\\nthese pros and cons in Table 1-2.\\nPro: Lower Cost\\nThe costs of storing data and executing queries on a data lake are much lower\\nthan in a data warehouse. This makes a data lake particularly useful for enabling\\nanalytics on data that isn’t high enough priority to justify the cost of a data\\nwarehouse enabling a wider analytical reach.\\n20 | Chapter 1: What Is Apache Iceberg?\\n\\n• A data warehouse could only store structured data •\\n•Storage in a data warehouse is generally more expensive than on-prem Hadoop•\\nclusters or cloud object storage.\\nTo address these issues the goal was to have an alternative storage solution that was\\ncheaper and could store all our data. This is what’s called the data lake.\\nOriginally, you’ d use a Hadoop to allow you to use a cluster of inexpensive comput‐\\ners to store large amounts of structured and unstructured data. Although it wasn’t\\nenough to just be able to store all this data. Y ou’ d want to run analytics on it too.\\nThe Hadoop ecosystem included MapReduce, an analytics framework from which\\nyou’ d write analytics jobs in Java and run them on the cluster. Many analysts are more\\ncomfortable writing SQL than Java, so Hive was created to convert SQL statements\\ninto MapReduce jobs.\\nTo write SQL, a mechanism to distinguish which files in our storage are part of the\\ndataset or table we want to run the SQL against was needed. This resulted in the birth\\nof the Hive table format which recognized a directory and the files inside it as a table.\\nOver time. people moved away from using Hadoop clusters to using Cloud Object\\nstorage as it was easier to manage and cheaper to use. MapReduce also fell out of\\nfavor for other distributed query engines like Apache Spark, Presto, and Dremio.\\nWhat did stick around was the Hive table format which became the standard in the\\nspace for recognizing files in your storage as singular tables on which you can run\\nanalytics.\\nA distinguishing feature of the data lake as compared to the data warehouse is\\nthe ability to leverage different compute engines for different workloads. This is\\nimportant because there’s never been a silver bullet of a compute engine that is best\\nfor every workload. This is just inherent to the nature of computing since there are\\nalways tradeoffs, and what you decide to tradeoff determines what a given system is\\ngood for and what it is not as well suited for.\\nNote that in data lakes, there isn’t really any service that fulfills the needs of the\\nstorage engine function. Generally the compute engine decides how to write the data,\\nthen the data is usually never revisited and optimized, unless rewriting entire tables\\nor partitions which is usually done on an ad-hoc basis. Refer to Figure 1-3 to see how\\nthe components of a data lake interact with one another.\\nData Lake | 19\\n\\nPro: Store Data in Open Formats\\nIn a data lake you can store the data in any file format you like unlike data\\nwarehouses where you have no say in how the data is stored, which would\\ntypically be a proprietary format built for that particular data warehouse. This\\nallows you to have more control over the data and consume the data in a greater\\nvariety of tools that can support these open formats.\\nPro: Handle Unstructured Data\\nData warehouses can’t handle unstructured data, so if you wanted to run analyt‐\\nics on unstructured data the data lake was the only option.\\nCon: Performance\\nSince each component of a data lake is decoupled, many of the optimizations\\nthat can exist in tightly coupled systems are absent. While they can be recreated,\\nit requires a lot of effort and engineering performant to cobble the components\\n(storage, file format, table format, engines) in a way to give you the comparable\\nperformance of a data warehouse. This made data lakes undesirable for high\\npriority data analytics where performance and time mattered.\\nCon: Lots of Con/f_iguration\\nAs previously mentioned, creating a tighter coupling of your chosen components\\nwith the level of optimizations you’ d expect from a data warehouse would require\\nsignificant engineering. This would result in a need for lots of data engineers to\\nconfigure all these tools, which can also be costly.\\nTable 1-2. Pros and cons of a data lake\\nPros Cons\\n• Lower Cost\\n• Store Data in Open Formats\\n• Handle unstructured data• Performance\\n• Lack of ACID Guarantees\\n• Lots of Con/f_igurations\\nShould I Run Analytics on the Data Lake or Data\\nWarehouse?\\nWhile Data Lakes provided a great place to land all your structured and unstructured\\ndata, there were still imperfections. After running ETL to land your data in your data\\nlake you’ d generally take one of two tracks when running analytics.\\nA Subset of Data goes to the Data Warehouse\\nY ou’ d set up an additional ETL pipeline to create a copy of a curated subset of\\ndata that is for high priority for analytics and store it in the warehouse to get the\\nperformance and flexibility of the data warehouse.\\nThis results in several issues:\\nShould I Run Analytics on the Data Lake or Data Warehouse? | 21\\n\\ndirectly without risking the system’s corruption. The details of how the data is stored\\nare abstracted away, and users take for granted that the platform knows where the\\ndata for a specific table is located and how to access it.\\nHowever, in today’s big data world, relying on a single closed engine to manage all\\naccess to the underlying data is no longer practical, as traditional RDBMSs are no\\nlonger sufficient.\\nIn a data lake, all your data is stored as files in some storage solution (e.g., Amazon\\nS3, Azure’s ADLS, Google’s GCS), so a single table may be made of dozens, hundreds,\\nor even thousands of individual files on that storage. When using SQL with our\\nfavorite analytical tools or writing ad hoc scripts in languages like Java, Scala, Python,\\nand Rust, we wouldn’t want to constantly define which of these files are in the table\\nand which of them aren’t. Not only would this be tedious but it would also likely lead\\nto inconsistency across different uses of the data.\\nSo the solution was to create a standard method of understanding “what data is in\\nthis table” for data lakes.\\nFigure 1-5. [Caption to come]\\nEarly Data Lake Table Formats\\nWhen it came to the world of running analytics on Hadoop data lakes, the Map‐\\nReduce framework was used which required users to write complex and tedious\\njava jobs, which wasn’t accessible to many analysts. Facebook, feeling the pain of\\nthis situation developed a framework called Hive in 2009. Hive provided a key\\nbenefit to make analytics on Hadoop much easier, the ability to write SQL instead of\\nMapReduce jobs directly.\\nThe Hive framework would take SQL statements and then convert them into Map‐\\nReduce jobs that can be executed. In order to write SQL statements, there had to\\nbe a mechanism for understanding what data on your Hadoop storage represented\\nEarly Data Lake Table Formats | 25\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 10:chain:format_inputs] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\",\n",
      "  \"context\": \"•Additional costs in the compute for the additional ETL work and the cost for•\\nstoring a copy of data you are already storing in a data warehouse where the\\nstorage costs are often greater.\\n•Additional copies of the data may be needed to populate data marts for•\\ndifferent business lines and even more copies as analysts create physical\\ncopies of data subsets in the form of BI extracts to speed up dashboards.\\nLeading to a web of data copies that are hard to govern, track and keep in\\nsync.\\nYou run analytics directly on the Data Lake\\nY ou’ d use query engines that support data lake workloads like Dremio, Presto,\\nApache Spark, Trino, Apache Impala and more to execute queries on the data\\nlake. These engines are generally well suited for read-only workloads. However,\\ndue to the limitations of the Hive table format, they ran into complexity when\\ntrying to update the data safely from the data lake.\\nSo the data lake and data warehouse each have their unique benefits and unique cons.\\nIt would be to our advantage to develop a new architecture that brings together all\\nthese benefits while minimizing all their faults, and that architecture is called a data\\nlakehouse.\\nEnter Data Lakehouse\\nWhile using a data warehouse gave us performance and ease of use, analytics on data\\nlakes gave us lower costs and reduced data drift from a complex web of data copies.\\nThe desire to thread the needle leads to great strides and innovation leading to what\\nwe now know as the data lakehouse.\\nWhat makes a data lakehouse truly unique are data lake table formats that eliminate\\nall the previous issues with the Hive table format. Y ou store the data in the same\\nplaces you would with a data lake, you use the query engines you would use with a\\ndata lake, your data is stored in the same formats it would be on a data lake, what\\ntruly transforms your world from a “read only” data to a “center of my data world”\\ndata lakehouse is the table format (refer to Figure 1-4). Table formats enabled better\\nconsistency, performance and ACID guarantees when working with data directly on\\nyour data lake storage leading to several value propositions.\\nFewer Copies, Less Dri/f_t\\nWith ACID guarantees and better performance you can now move workloads\\ntypically saved for the data warehouse like updates and other data manipulation.\\nIf you don’t have to move your data to the lakehouse you can have a more\\nstreamlined architecture with fewer copies. Fewer copies mean less storage costs,\\nless compute costs from moving data to a data warehouse, and better governance\\nof your data to maintain compliance with regulations and internal controls.\\n22 | Chapter 1: What Is Apache Iceberg?\\n\\nFigure 1-3. Technical components in a data lake\\nPros and Cons of a Data Lake\\nNo architectural pattern is perfect and that applies to data lakes. While data lakes\\nhave a lot of benefits like Lower Costs, the ability to store in open formats and handle\\nunstructured data; data lakes also have several disadvantages such as performance\\nissues, lack of ACID guarantees and lots of configuration. Y ou can see a summary of\\nthese pros and cons in Table 1-2.\\nPro: Lower Cost\\nThe costs of storing data and executing queries on a data lake are much lower\\nthan in a data warehouse. This makes a data lake particularly useful for enabling\\nanalytics on data that isn’t high enough priority to justify the cost of a data\\nwarehouse enabling a wider analytical reach.\\n20 | Chapter 1: What Is Apache Iceberg?\\n\\n• A data warehouse could only store structured data •\\n•Storage in a data warehouse is generally more expensive than on-prem Hadoop•\\nclusters or cloud object storage.\\nTo address these issues the goal was to have an alternative storage solution that was\\ncheaper and could store all our data. This is what’s called the data lake.\\nOriginally, you’ d use a Hadoop to allow you to use a cluster of inexpensive comput‐\\ners to store large amounts of structured and unstructured data. Although it wasn’t\\nenough to just be able to store all this data. Y ou’ d want to run analytics on it too.\\nThe Hadoop ecosystem included MapReduce, an analytics framework from which\\nyou’ d write analytics jobs in Java and run them on the cluster. Many analysts are more\\ncomfortable writing SQL than Java, so Hive was created to convert SQL statements\\ninto MapReduce jobs.\\nTo write SQL, a mechanism to distinguish which files in our storage are part of the\\ndataset or table we want to run the SQL against was needed. This resulted in the birth\\nof the Hive table format which recognized a directory and the files inside it as a table.\\nOver time. people moved away from using Hadoop clusters to using Cloud Object\\nstorage as it was easier to manage and cheaper to use. MapReduce also fell out of\\nfavor for other distributed query engines like Apache Spark, Presto, and Dremio.\\nWhat did stick around was the Hive table format which became the standard in the\\nspace for recognizing files in your storage as singular tables on which you can run\\nanalytics.\\nA distinguishing feature of the data lake as compared to the data warehouse is\\nthe ability to leverage different compute engines for different workloads. This is\\nimportant because there’s never been a silver bullet of a compute engine that is best\\nfor every workload. This is just inherent to the nature of computing since there are\\nalways tradeoffs, and what you decide to tradeoff determines what a given system is\\ngood for and what it is not as well suited for.\\nNote that in data lakes, there isn’t really any service that fulfills the needs of the\\nstorage engine function. Generally the compute engine decides how to write the data,\\nthen the data is usually never revisited and optimized, unless rewriting entire tables\\nor partitions which is usually done on an ad-hoc basis. Refer to Figure 1-3 to see how\\nthe components of a data lake interact with one another.\\nData Lake | 19\\n\\nPro: Store Data in Open Formats\\nIn a data lake you can store the data in any file format you like unlike data\\nwarehouses where you have no say in how the data is stored, which would\\ntypically be a proprietary format built for that particular data warehouse. This\\nallows you to have more control over the data and consume the data in a greater\\nvariety of tools that can support these open formats.\\nPro: Handle Unstructured Data\\nData warehouses can’t handle unstructured data, so if you wanted to run analyt‐\\nics on unstructured data the data lake was the only option.\\nCon: Performance\\nSince each component of a data lake is decoupled, many of the optimizations\\nthat can exist in tightly coupled systems are absent. While they can be recreated,\\nit requires a lot of effort and engineering performant to cobble the components\\n(storage, file format, table format, engines) in a way to give you the comparable\\nperformance of a data warehouse. This made data lakes undesirable for high\\npriority data analytics where performance and time mattered.\\nCon: Lots of Con/f_iguration\\nAs previously mentioned, creating a tighter coupling of your chosen components\\nwith the level of optimizations you’ d expect from a data warehouse would require\\nsignificant engineering. This would result in a need for lots of data engineers to\\nconfigure all these tools, which can also be costly.\\nTable 1-2. Pros and cons of a data lake\\nPros Cons\\n• Lower Cost\\n• Store Data in Open Formats\\n• Handle unstructured data• Performance\\n• Lack of ACID Guarantees\\n• Lots of Con/f_igurations\\nShould I Run Analytics on the Data Lake or Data\\nWarehouse?\\nWhile Data Lakes provided a great place to land all your structured and unstructured\\ndata, there were still imperfections. After running ETL to land your data in your data\\nlake you’ d generally take one of two tracks when running analytics.\\nA Subset of Data goes to the Data Warehouse\\nY ou’ d set up an additional ETL pipeline to create a copy of a curated subset of\\ndata that is for high priority for analytics and store it in the warehouse to get the\\nperformance and flexibility of the data warehouse.\\nThis results in several issues:\\nShould I Run Analytics on the Data Lake or Data Warehouse? | 21\\n\\ndirectly without risking the system’s corruption. The details of how the data is stored\\nare abstracted away, and users take for granted that the platform knows where the\\ndata for a specific table is located and how to access it.\\nHowever, in today’s big data world, relying on a single closed engine to manage all\\naccess to the underlying data is no longer practical, as traditional RDBMSs are no\\nlonger sufficient.\\nIn a data lake, all your data is stored as files in some storage solution (e.g., Amazon\\nS3, Azure’s ADLS, Google’s GCS), so a single table may be made of dozens, hundreds,\\nor even thousands of individual files on that storage. When using SQL with our\\nfavorite analytical tools or writing ad hoc scripts in languages like Java, Scala, Python,\\nand Rust, we wouldn’t want to constantly define which of these files are in the table\\nand which of them aren’t. Not only would this be tedious but it would also likely lead\\nto inconsistency across different uses of the data.\\nSo the solution was to create a standard method of understanding “what data is in\\nthis table” for data lakes.\\nFigure 1-5. [Caption to come]\\nEarly Data Lake Table Formats\\nWhen it came to the world of running analytics on Hadoop data lakes, the Map‐\\nReduce framework was used which required users to write complex and tedious\\njava jobs, which wasn’t accessible to many analysts. Facebook, feeling the pain of\\nthis situation developed a framework called Hive in 2009. Hive provided a key\\nbenefit to make analytics on Hadoop much easier, the ability to write SQL instead of\\nMapReduce jobs directly.\\nThe Hive framework would take SQL statements and then convert them into Map‐\\nReduce jobs that can be executed. In order to write SQL statements, there had to\\nbe a mechanism for understanding what data on your Hadoop storage represented\\nEarly Data Lake Table Formats | 25\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 13:prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is data lake?\",\n",
      "  \"context\": \"•Additional costs in the compute for the additional ETL work and the cost for•\\nstoring a copy of data you are already storing in a data warehouse where the\\nstorage costs are often greater.\\n•Additional copies of the data may be needed to populate data marts for•\\ndifferent business lines and even more copies as analysts create physical\\ncopies of data subsets in the form of BI extracts to speed up dashboards.\\nLeading to a web of data copies that are hard to govern, track and keep in\\nsync.\\nYou run analytics directly on the Data Lake\\nY ou’ d use query engines that support data lake workloads like Dremio, Presto,\\nApache Spark, Trino, Apache Impala and more to execute queries on the data\\nlake. These engines are generally well suited for read-only workloads. However,\\ndue to the limitations of the Hive table format, they ran into complexity when\\ntrying to update the data safely from the data lake.\\nSo the data lake and data warehouse each have their unique benefits and unique cons.\\nIt would be to our advantage to develop a new architecture that brings together all\\nthese benefits while minimizing all their faults, and that architecture is called a data\\nlakehouse.\\nEnter Data Lakehouse\\nWhile using a data warehouse gave us performance and ease of use, analytics on data\\nlakes gave us lower costs and reduced data drift from a complex web of data copies.\\nThe desire to thread the needle leads to great strides and innovation leading to what\\nwe now know as the data lakehouse.\\nWhat makes a data lakehouse truly unique are data lake table formats that eliminate\\nall the previous issues with the Hive table format. Y ou store the data in the same\\nplaces you would with a data lake, you use the query engines you would use with a\\ndata lake, your data is stored in the same formats it would be on a data lake, what\\ntruly transforms your world from a “read only” data to a “center of my data world”\\ndata lakehouse is the table format (refer to Figure 1-4). Table formats enabled better\\nconsistency, performance and ACID guarantees when working with data directly on\\nyour data lake storage leading to several value propositions.\\nFewer Copies, Less Dri/f_t\\nWith ACID guarantees and better performance you can now move workloads\\ntypically saved for the data warehouse like updates and other data manipulation.\\nIf you don’t have to move your data to the lakehouse you can have a more\\nstreamlined architecture with fewer copies. Fewer copies mean less storage costs,\\nless compute costs from moving data to a data warehouse, and better governance\\nof your data to maintain compliance with regulations and internal controls.\\n22 | Chapter 1: What Is Apache Iceberg?\\n\\nFigure 1-3. Technical components in a data lake\\nPros and Cons of a Data Lake\\nNo architectural pattern is perfect and that applies to data lakes. While data lakes\\nhave a lot of benefits like Lower Costs, the ability to store in open formats and handle\\nunstructured data; data lakes also have several disadvantages such as performance\\nissues, lack of ACID guarantees and lots of configuration. Y ou can see a summary of\\nthese pros and cons in Table 1-2.\\nPro: Lower Cost\\nThe costs of storing data and executing queries on a data lake are much lower\\nthan in a data warehouse. This makes a data lake particularly useful for enabling\\nanalytics on data that isn’t high enough priority to justify the cost of a data\\nwarehouse enabling a wider analytical reach.\\n20 | Chapter 1: What Is Apache Iceberg?\\n\\n• A data warehouse could only store structured data •\\n•Storage in a data warehouse is generally more expensive than on-prem Hadoop•\\nclusters or cloud object storage.\\nTo address these issues the goal was to have an alternative storage solution that was\\ncheaper and could store all our data. This is what’s called the data lake.\\nOriginally, you’ d use a Hadoop to allow you to use a cluster of inexpensive comput‐\\ners to store large amounts of structured and unstructured data. Although it wasn’t\\nenough to just be able to store all this data. Y ou’ d want to run analytics on it too.\\nThe Hadoop ecosystem included MapReduce, an analytics framework from which\\nyou’ d write analytics jobs in Java and run them on the cluster. Many analysts are more\\ncomfortable writing SQL than Java, so Hive was created to convert SQL statements\\ninto MapReduce jobs.\\nTo write SQL, a mechanism to distinguish which files in our storage are part of the\\ndataset or table we want to run the SQL against was needed. This resulted in the birth\\nof the Hive table format which recognized a directory and the files inside it as a table.\\nOver time. people moved away from using Hadoop clusters to using Cloud Object\\nstorage as it was easier to manage and cheaper to use. MapReduce also fell out of\\nfavor for other distributed query engines like Apache Spark, Presto, and Dremio.\\nWhat did stick around was the Hive table format which became the standard in the\\nspace for recognizing files in your storage as singular tables on which you can run\\nanalytics.\\nA distinguishing feature of the data lake as compared to the data warehouse is\\nthe ability to leverage different compute engines for different workloads. This is\\nimportant because there’s never been a silver bullet of a compute engine that is best\\nfor every workload. This is just inherent to the nature of computing since there are\\nalways tradeoffs, and what you decide to tradeoff determines what a given system is\\ngood for and what it is not as well suited for.\\nNote that in data lakes, there isn’t really any service that fulfills the needs of the\\nstorage engine function. Generally the compute engine decides how to write the data,\\nthen the data is usually never revisited and optimized, unless rewriting entire tables\\nor partitions which is usually done on an ad-hoc basis. Refer to Figure 1-3 to see how\\nthe components of a data lake interact with one another.\\nData Lake | 19\\n\\nPro: Store Data in Open Formats\\nIn a data lake you can store the data in any file format you like unlike data\\nwarehouses where you have no say in how the data is stored, which would\\ntypically be a proprietary format built for that particular data warehouse. This\\nallows you to have more control over the data and consume the data in a greater\\nvariety of tools that can support these open formats.\\nPro: Handle Unstructured Data\\nData warehouses can’t handle unstructured data, so if you wanted to run analyt‐\\nics on unstructured data the data lake was the only option.\\nCon: Performance\\nSince each component of a data lake is decoupled, many of the optimizations\\nthat can exist in tightly coupled systems are absent. While they can be recreated,\\nit requires a lot of effort and engineering performant to cobble the components\\n(storage, file format, table format, engines) in a way to give you the comparable\\nperformance of a data warehouse. This made data lakes undesirable for high\\npriority data analytics where performance and time mattered.\\nCon: Lots of Con/f_iguration\\nAs previously mentioned, creating a tighter coupling of your chosen components\\nwith the level of optimizations you’ d expect from a data warehouse would require\\nsignificant engineering. This would result in a need for lots of data engineers to\\nconfigure all these tools, which can also be costly.\\nTable 1-2. Pros and cons of a data lake\\nPros Cons\\n• Lower Cost\\n• Store Data in Open Formats\\n• Handle unstructured data• Performance\\n• Lack of ACID Guarantees\\n• Lots of Con/f_igurations\\nShould I Run Analytics on the Data Lake or Data\\nWarehouse?\\nWhile Data Lakes provided a great place to land all your structured and unstructured\\ndata, there were still imperfections. After running ETL to land your data in your data\\nlake you’ d generally take one of two tracks when running analytics.\\nA Subset of Data goes to the Data Warehouse\\nY ou’ d set up an additional ETL pipeline to create a copy of a curated subset of\\ndata that is for high priority for analytics and store it in the warehouse to get the\\nperformance and flexibility of the data warehouse.\\nThis results in several issues:\\nShould I Run Analytics on the Data Lake or Data Warehouse? | 21\\n\\ndirectly without risking the system’s corruption. The details of how the data is stored\\nare abstracted away, and users take for granted that the platform knows where the\\ndata for a specific table is located and how to access it.\\nHowever, in today’s big data world, relying on a single closed engine to manage all\\naccess to the underlying data is no longer practical, as traditional RDBMSs are no\\nlonger sufficient.\\nIn a data lake, all your data is stored as files in some storage solution (e.g., Amazon\\nS3, Azure’s ADLS, Google’s GCS), so a single table may be made of dozens, hundreds,\\nor even thousands of individual files on that storage. When using SQL with our\\nfavorite analytical tools or writing ad hoc scripts in languages like Java, Scala, Python,\\nand Rust, we wouldn’t want to constantly define which of these files are in the table\\nand which of them aren’t. Not only would this be tedious but it would also likely lead\\nto inconsistency across different uses of the data.\\nSo the solution was to create a standard method of understanding “what data is in\\nthis table” for data lakes.\\nFigure 1-5. [Caption to come]\\nEarly Data Lake Table Formats\\nWhen it came to the world of running analytics on Hadoop data lakes, the Map‐\\nReduce framework was used which required users to write complex and tedious\\njava jobs, which wasn’t accessible to many analysts. Facebook, feeling the pain of\\nthis situation developed a framework called Hive in 2009. Hive provided a key\\nbenefit to make analytics on Hadoop much easier, the ability to write SQL instead of\\nMapReduce jobs directly.\\nThe Hive framework would take SQL statements and then convert them into Map‐\\nReduce jobs that can be executed. In order to write SQL statements, there had to\\nbe a mechanism for understanding what data on your Hadoop storage represented\\nEarly Data Lake Table Formats | 25\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 13:prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 14:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nAnswer the question based on the context below. If you can't \\nanswer the question, reply \\\"I don't know\\\".\\nContext: •Additional costs in the compute for the additional ETL work and the cost for•\\nstoring a copy of data you are already storing in a data warehouse where the\\nstorage costs are often greater.\\n•Additional copies of the data may be needed to populate data marts for•\\ndifferent business lines and even more copies as analysts create physical\\ncopies of data subsets in the form of BI extracts to speed up dashboards.\\nLeading to a web of data copies that are hard to govern, track and keep in\\nsync.\\nYou run analytics directly on the Data Lake\\nY ou’ d use query engines that support data lake workloads like Dremio, Presto,\\nApache Spark, Trino, Apache Impala and more to execute queries on the data\\nlake. These engines are generally well suited for read-only workloads. However,\\ndue to the limitations of the Hive table format, they ran into complexity when\\ntrying to update the data safely from the data lake.\\nSo the data lake and data warehouse each have their unique benefits and unique cons.\\nIt would be to our advantage to develop a new architecture that brings together all\\nthese benefits while minimizing all their faults, and that architecture is called a data\\nlakehouse.\\nEnter Data Lakehouse\\nWhile using a data warehouse gave us performance and ease of use, analytics on data\\nlakes gave us lower costs and reduced data drift from a complex web of data copies.\\nThe desire to thread the needle leads to great strides and innovation leading to what\\nwe now know as the data lakehouse.\\nWhat makes a data lakehouse truly unique are data lake table formats that eliminate\\nall the previous issues with the Hive table format. Y ou store the data in the same\\nplaces you would with a data lake, you use the query engines you would use with a\\ndata lake, your data is stored in the same formats it would be on a data lake, what\\ntruly transforms your world from a “read only” data to a “center of my data world”\\ndata lakehouse is the table format (refer to Figure 1-4). Table formats enabled better\\nconsistency, performance and ACID guarantees when working with data directly on\\nyour data lake storage leading to several value propositions.\\nFewer Copies, Less Dri/f_t\\nWith ACID guarantees and better performance you can now move workloads\\ntypically saved for the data warehouse like updates and other data manipulation.\\nIf you don’t have to move your data to the lakehouse you can have a more\\nstreamlined architecture with fewer copies. Fewer copies mean less storage costs,\\nless compute costs from moving data to a data warehouse, and better governance\\nof your data to maintain compliance with regulations and internal controls.\\n22 | Chapter 1: What Is Apache Iceberg?\\n\\nFigure 1-3. Technical components in a data lake\\nPros and Cons of a Data Lake\\nNo architectural pattern is perfect and that applies to data lakes. While data lakes\\nhave a lot of benefits like Lower Costs, the ability to store in open formats and handle\\nunstructured data; data lakes also have several disadvantages such as performance\\nissues, lack of ACID guarantees and lots of configuration. Y ou can see a summary of\\nthese pros and cons in Table 1-2.\\nPro: Lower Cost\\nThe costs of storing data and executing queries on a data lake are much lower\\nthan in a data warehouse. This makes a data lake particularly useful for enabling\\nanalytics on data that isn’t high enough priority to justify the cost of a data\\nwarehouse enabling a wider analytical reach.\\n20 | Chapter 1: What Is Apache Iceberg?\\n\\n• A data warehouse could only store structured data •\\n•Storage in a data warehouse is generally more expensive than on-prem Hadoop•\\nclusters or cloud object storage.\\nTo address these issues the goal was to have an alternative storage solution that was\\ncheaper and could store all our data. This is what’s called the data lake.\\nOriginally, you’ d use a Hadoop to allow you to use a cluster of inexpensive comput‐\\ners to store large amounts of structured and unstructured data. Although it wasn’t\\nenough to just be able to store all this data. Y ou’ d want to run analytics on it too.\\nThe Hadoop ecosystem included MapReduce, an analytics framework from which\\nyou’ d write analytics jobs in Java and run them on the cluster. Many analysts are more\\ncomfortable writing SQL than Java, so Hive was created to convert SQL statements\\ninto MapReduce jobs.\\nTo write SQL, a mechanism to distinguish which files in our storage are part of the\\ndataset or table we want to run the SQL against was needed. This resulted in the birth\\nof the Hive table format which recognized a directory and the files inside it as a table.\\nOver time. people moved away from using Hadoop clusters to using Cloud Object\\nstorage as it was easier to manage and cheaper to use. MapReduce also fell out of\\nfavor for other distributed query engines like Apache Spark, Presto, and Dremio.\\nWhat did stick around was the Hive table format which became the standard in the\\nspace for recognizing files in your storage as singular tables on which you can run\\nanalytics.\\nA distinguishing feature of the data lake as compared to the data warehouse is\\nthe ability to leverage different compute engines for different workloads. This is\\nimportant because there’s never been a silver bullet of a compute engine that is best\\nfor every workload. This is just inherent to the nature of computing since there are\\nalways tradeoffs, and what you decide to tradeoff determines what a given system is\\ngood for and what it is not as well suited for.\\nNote that in data lakes, there isn’t really any service that fulfills the needs of the\\nstorage engine function. Generally the compute engine decides how to write the data,\\nthen the data is usually never revisited and optimized, unless rewriting entire tables\\nor partitions which is usually done on an ad-hoc basis. Refer to Figure 1-3 to see how\\nthe components of a data lake interact with one another.\\nData Lake | 19\\n\\nPro: Store Data in Open Formats\\nIn a data lake you can store the data in any file format you like unlike data\\nwarehouses where you have no say in how the data is stored, which would\\ntypically be a proprietary format built for that particular data warehouse. This\\nallows you to have more control over the data and consume the data in a greater\\nvariety of tools that can support these open formats.\\nPro: Handle Unstructured Data\\nData warehouses can’t handle unstructured data, so if you wanted to run analyt‐\\nics on unstructured data the data lake was the only option.\\nCon: Performance\\nSince each component of a data lake is decoupled, many of the optimizations\\nthat can exist in tightly coupled systems are absent. While they can be recreated,\\nit requires a lot of effort and engineering performant to cobble the components\\n(storage, file format, table format, engines) in a way to give you the comparable\\nperformance of a data warehouse. This made data lakes undesirable for high\\npriority data analytics where performance and time mattered.\\nCon: Lots of Con/f_iguration\\nAs previously mentioned, creating a tighter coupling of your chosen components\\nwith the level of optimizations you’ d expect from a data warehouse would require\\nsignificant engineering. This would result in a need for lots of data engineers to\\nconfigure all these tools, which can also be costly.\\nTable 1-2. Pros and cons of a data lake\\nPros Cons\\n• Lower Cost\\n• Store Data in Open Formats\\n• Handle unstructured data• Performance\\n• Lack of ACID Guarantees\\n• Lots of Con/f_igurations\\nShould I Run Analytics on the Data Lake or Data\\nWarehouse?\\nWhile Data Lakes provided a great place to land all your structured and unstructured\\ndata, there were still imperfections. After running ETL to land your data in your data\\nlake you’ d generally take one of two tracks when running analytics.\\nA Subset of Data goes to the Data Warehouse\\nY ou’ d set up an additional ETL pipeline to create a copy of a curated subset of\\ndata that is for high priority for analytics and store it in the warehouse to get the\\nperformance and flexibility of the data warehouse.\\nThis results in several issues:\\nShould I Run Analytics on the Data Lake or Data Warehouse? | 21\\n\\ndirectly without risking the system’s corruption. The details of how the data is stored\\nare abstracted away, and users take for granted that the platform knows where the\\ndata for a specific table is located and how to access it.\\nHowever, in today’s big data world, relying on a single closed engine to manage all\\naccess to the underlying data is no longer practical, as traditional RDBMSs are no\\nlonger sufficient.\\nIn a data lake, all your data is stored as files in some storage solution (e.g., Amazon\\nS3, Azure’s ADLS, Google’s GCS), so a single table may be made of dozens, hundreds,\\nor even thousands of individual files on that storage. When using SQL with our\\nfavorite analytical tools or writing ad hoc scripts in languages like Java, Scala, Python,\\nand Rust, we wouldn’t want to constantly define which of these files are in the table\\nand which of them aren’t. Not only would this be tedious but it would also likely lead\\nto inconsistency across different uses of the data.\\nSo the solution was to create a standard method of understanding “what data is in\\nthis table” for data lakes.\\nFigure 1-5. [Caption to come]\\nEarly Data Lake Table Formats\\nWhen it came to the world of running analytics on Hadoop data lakes, the Map‐\\nReduce framework was used which required users to write complex and tedious\\njava jobs, which wasn’t accessible to many analysts. Facebook, feeling the pain of\\nthis situation developed a framework called Hive in 2009. Hive provided a key\\nbenefit to make analytics on Hadoop much easier, the ability to write SQL instead of\\nMapReduce jobs directly.\\nThe Hive framework would take SQL statements and then convert them into Map‐\\nReduce jobs that can be executed. In order to write SQL statements, there had to\\nbe a mechanism for understanding what data on your Hadoop storage represented\\nEarly Data Lake Table Formats | 25\\nQuestion: What is data lake?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 14:llm:ChatOpenAI] [2.48s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"A data lake is a storage system that allows you to store large amounts of structured and unstructured data in its original format, providing lower costs and the ability to handle open formats and unstructured data.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"A data lake is a storage system that allows you to store large amounts of structured and unstructured data in its original format, providing lower costs and the ability to handle open formats and unstructured data.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"name\": null,\n",
      "            \"id\": null\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 40,\n",
      "      \"prompt_tokens\": 2182,\n",
      "      \"total_tokens\": 2222\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_3bc1b5746c\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 15:parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain > 15:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A data lake is a storage system that allows you to store large amounts of structured and unstructured data in its original format, providing lower costs and the ability to handle open formats and unstructured data.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer> > 9:chain:stuff_documents_chain] [2.48s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A data lake is a storage system that allows you to store large amounts of structured and unstructured data in its original format, providing lower costs and the ability to handle open formats and unstructured data.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer> > 8:chain:RunnableParallel<answer>] [2.49s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"A data lake is a storage system that allows you to store large amounts of structured and unstructured data in its original format, providing lower costs and the ability to handle open formats and unstructured data.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain > 7:chain:RunnableAssign<answer>] [2.49s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:retrieval_chain] [2.57s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "A data lake is a storage system that allows you to store large amounts of structured and unstructured data in its original format, providing lower costs and the ability to handle open formats and unstructured data.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.globals import set_debug\n",
    "from langchain.prompts import PromptTemplate\n",
    "set_debug(True)\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "#Invoke the retrieval chain\n",
    "response=retrieval_chain.invoke({\"input\":\"What is data lake?\"})\n",
    "\n",
    "#Print the answer to the question\n",
    "print(response[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
